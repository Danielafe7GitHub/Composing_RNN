{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando Paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk, re, time\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2   Mortar assault leaves at least 18 dead\n",
      "\n",
      "Labels:  3695 Sentences:  3695\n"
     ]
    }
   ],
   "source": [
    "#--------------------Extracting Emotion && sentences from corpus\n",
    "def extract(dataset):\n",
    "    f = open(dataset, 'r+')\n",
    "    linea = f.readline()\n",
    "    emotion = []\n",
    "    sentences = []\n",
    "    neutro = re.compile('^ne')\n",
    "    while linea != \"\":    \n",
    "        #Ignoramos Neutro emotions\n",
    "        if not re.match(neutro, linea):\n",
    "            #print(\"** \",linea)\n",
    "            linea = linea.split(\"#\")\n",
    "            #Obtaining the emocion\n",
    "            emotion.append(int(linea[0]))\n",
    "            sentences.append(linea[1])\n",
    "        linea = f.readline()\n",
    "    f.close()\n",
    "    return sentences,emotion\n",
    "\n",
    "dataset = \"Heterogenious_Dataset/dataset.txt\"\n",
    "sentences,emotion = extract(dataset)\n",
    "print(emotion[0],\" \",sentences[0])\n",
    "print(\"Labels: \",len(emotion), \"Sentences: \",len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords=True):\n",
    "    '''Clean the text, with the option to remove stopwords'''\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"<br />\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z]\", \" \", text)\n",
    "    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n",
    "    text = re.sub(r\"  \", \" \", text)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = []\n",
    "for sentence in sentences:\n",
    "    data_clean.append(clean_text(review))\n",
    "    \n",
    "test_clean = []\n",
    "for review in test.review:\n",
    "    test_clean.append(clean_text(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stuff going moment mj i ve started listening music watching odd documentary there watched wiz watched moonwalker again maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj s feeling towards press also obvious message drugs bad m kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice him the actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond me mj overheard plans nah joe pesci s character ranted wanted people know supplying drugs etc dunno maybe hates mj s music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another which think people not stay away try give wholesome message ironically mj s bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention i ve gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter \n",
      "\n",
      " the classic war worlds timothy hines entertaining film obviously goes great effort lengths faithfully recreate h g wells classic book mr hines succeeds so i watched film me appreciated fact standard predictable hollywood fare comes every year e g spielberg version tom cruise slightest resemblance book obviously everyone looks different things movie envision amateur critics look criticize everything can others rate movie important bases like entertained people never agree critics enjoyed effort mr hines put faithful h g wells classic novel found entertaining made easy overlook critics perceive shortcomings \n",
      "\n",
      "film starts manager nicholas bell giving welcome investors robert carradine primal park secret project mutating primal animal using fossilized dna like jurassik park scientists resurrect one nature s fearsome predators sabretooth tiger smilodon scientific ambition turns deadly however high voltage fence opened creature escape begins savagely stalking prey human visitors tourists scientific meanwhile youngsters enter restricted area security center attacked pack large pre historical animals deadlier bigger addition security agent stacy haiduk mate brian wimmer fight hardly carnivorous smilodons sabretooths course real star stars astounding terrifyingly though convincing giant animals savagely stalking prey group run afoul fight one nature s fearsome predators furthermore third sabretooth dangerous slow stalks victims the movie delivers goods lots blood gore beheading hair raising chills full scares sabretooths appear mediocre special effects the story provides exciting stirring entertainment results quite boring the giant animals majority made computer generator seem totally lousy middling performances though players reacting appropriately becoming food actors give vigorously physical performances dodging beasts running bound leaps dangling walls packs ridiculous final deadly scene small kids realistic gory violent attack scenes films sabretooths smilodon following sabretooth  by james r hickox vanessa angel david keith john rhys davies much better  bc  roland emmerich steven strait cliff curtis camilla belle motion picture filled bloody moments badly directed george miller originality takes many elements previous films miller australian director usually working television tidal wave journey center earth many others occasionally cinema man snowy river zeus roxanne robinson crusoe rating average bottom barrel \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the cleaned reviews\n",
    "for i in range(3):\n",
    "    print(train_clean[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting is complete.\n",
      "train_seq is complete.\n",
      "test_seq is complete\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the reviews\n",
    "all_reviews = train_clean + test_clean\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_reviews)\n",
    "print(\"Fitting is complete.\")\n",
    "\n",
    "train_seq = tokenizer.texts_to_sequences(train_clean)\n",
    "print(\"train_seq is complete.\")\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_clean)\n",
    "print(\"test_seq is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in index: 99425\n"
     ]
    }
   ],
   "source": [
    "# Find the number of unique tokens\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Words in index: %d\" % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[437, 81, 481, 10863, 6, 71, 573, 2590, 115, 65, 948, 551, 51, 207, 24383, 207, 17034, 213, 188, 92, 19, 684, 2550, 118, 104, 14, 511, 3933, 188, 25, 240, 644, 2336, 1251, 17034, 85, 4772, 85, 701, 3, 298, 81, 15, 351, 1827, 533, 1209, 3566, 10863, 1, 477, 861, 3526, 22, 517, 662, 1403, 18, 60, 5290, 2073, 1109, 180, 406, 1512, 807, 2559, 5, 10863, 469, 81, 655, 80, 265, 109, 569, 10863, 33435, 29469, 141, 2, 10863, 374, 12, 57, 24, 374, 205, 14, 246, 173, 9, 740, 701, 3, 135, 334, 456, 138, 16333, 4102, 1702, 626, 865, 10467, 1009, 11946, 880, 1058, 1640, 408, 10863, 258, 18, 584, 134, 10863, 17899, 2288, 15681, 865, 10467, 1, 32, 36026, 381, 20, 46, 17447, 1403, 426, 9779, 188, 4220, 10863, 1, 115, 658, 511, 91, 5, 10863, 1541, 436, 2257, 131, 2124, 2367, 626, 22, 67, 112, 4731, 5337, 300, 1313, 29470, 18, 626, 547, 878, 655, 685, 4, 444, 190, 537, 131, 677, 3371, 1224, 779, 54, 1229, 260, 2, 20, 5, 10863, 4, 570, 73, 468, 30, 20, 239, 695, 151, 269, 108, 7617, 662, 3514, 10863, 1, 36027, 1676, 2, 152, 406, 1512, 287, 4, 946, 20, 48, 1488, 1215, 2336, 16, 601, 6, 71, 434, 713, 7167, 16, 46, 20, 194, 435, 3890, 3466, 46, 105, 263, 487, 246, 282, 118, 4, 19320, 19321, 355, 1489]\n",
      "\n",
      "[9, 272, 212, 3549, 4509, 10468, 348, 3, 452, 183, 21, 697, 8411, 10998, 8646, 1850, 1081, 4599, 272, 184, 359, 10468, 2736, 177, 6, 207, 3, 134, 2507, 105, 1190, 637, 262, 2544, 187, 84, 208, 669, 1081, 3317, 232, 746, 3719, 3878, 4315, 184, 452, 209, 203, 194, 91, 2, 19917, 2368, 1337, 79, 6462, 181, 87, 317, 884, 2, 567, 12471, 5, 2241, 20, 40, 941, 1337, 431, 697, 359, 10468, 186, 2779, 1850, 1081, 4599, 272, 591, 167, 348, 24, 680, 5218, 1337, 10999, 5607]\n",
      "\n",
      "[3, 456, 2875, 4773, 3612, 659, 2478, 17035, 528, 4475, 10238, 1147, 928, 1053, 31269, 10238, 1626, 707, 50267, 10239, 5, 50268, 1147, 3015, 11298, 4, 826, 1, 17900, 9575, 14531, 5037, 43715, 3623, 5338, 432, 2413, 102, 214, 26605, 7112, 2930, 1469, 962, 751, 17036, 6288, 4709, 308, 9361, 8885, 3623, 1933, 7431, 2248, 11138, 1524, 2479, 2087, 2931, 3016, 949, 1709, 1242, 1513, 33436, 1851, 1594, 2479, 1395, 9576, 36028, 4020, 1686, 50269, 476, 951, 23473, 62125, 31270, 180, 63, 237, 321, 5665, 20502, 70, 1009, 1253, 1513, 17036, 6288, 4709, 463, 412, 16334, 476, 4, 826, 1, 17900, 9575, 3791, 791, 14531, 1661, 480, 9577, 1450, 9, 2, 1467, 6243, 658, 464, 520, 17037, 1021, 5060, 5630, 279, 2668, 31270, 896, 1496, 222, 202, 9, 13, 1525, 1064, 8047, 639, 1936, 93, 265, 9, 1253, 1513, 2177, 24, 1213, 19918, 226, 377, 2198, 21883, 285, 70, 1800, 12472, 5083, 1530, 1596, 66, 108, 19919, 1646, 285, 16676, 12657, 554, 2685, 9163, 12855, 3613, 6966, 564, 388, 2413, 54, 316, 264, 722, 1994, 1046, 1275, 58, 33, 31270, 43715, 888, 14531, 959, 523, 1166, 31271, 6498, 2190, 512, 4021, 223, 8183, 3842, 17, 52, 16335, 9900, 36029, 2025, 16677, 3994, 4092, 8113, 6499, 1217, 353, 983, 1627, 304, 845, 442, 645, 2651, 2686, 221, 36, 730, 848, 33, 2651, 2038, 67, 547, 685, 608, 22662, 3105, 1170, 2087, 611, 36, 317, 1847, 351, 50, 9258, 1828, 18340, 27876, 4344, 43716, 589, 763, 1229, 5262]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the reviews after they have been tokenized\n",
    "for i in range(3):\n",
    "    print(train_seq[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the length of reviews\n",
    "lengths = []\n",
    "for review in train_seq:\n",
    "    lengths.append(len(review))\n",
    "\n",
    "for review in test_seq:\n",
    "    lengths.append(len(review))\n",
    "\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50000.000000\n",
       "mean       128.366580\n",
       "std         96.950296\n",
       "min          3.000000\n",
       "25%         68.000000\n",
       "50%         95.000000\n",
       "75%        156.000000\n",
       "max       1488.000000\n",
       "Name: counts, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.counts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178.0\n",
      "208.0\n",
      "253.0\n",
      "332.0\n"
     ]
    }
   ],
   "source": [
    "print(np.percentile(lengths.counts, 80))\n",
    "print(np.percentile(lengths.counts, 85))\n",
    "print(np.percentile(lengths.counts, 90))\n",
    "print(np.percentile(lengths.counts, 95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use 200 as the maximum length of a review. Although longer would be better, I want to limit the training time, and this value will still provide the full text to more than 80% of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_pad is complete.\n",
      "test_pad is complete.\n"
     ]
    }
   ],
   "source": [
    "# Pad and truncate the questions so that they all have the same length.\n",
    "max_review_length = 200\n",
    "\n",
    "train_pad = pad_sequences(train_seq, maxlen = max_review_length)\n",
    "print(\"train_pad is complete.\")\n",
    "\n",
    "test_pad = pad_sequences(test_seq, maxlen = max_review_length)\n",
    "print(\"test_pad is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   85  4772    85   701     3   298    81    15   351  1827   533  1209\n",
      "  3566 10863     1   477   861  3526    22   517   662  1403    18    60\n",
      "  5290  2073  1109   180   406  1512   807  2559     5 10863   469    81\n",
      "   655    80   265   109   569 10863 33435 29469   141     2 10863   374\n",
      "    12    57    24   374   205    14   246   173     9   740   701     3\n",
      "   135   334   456   138 16333  4102  1702   626   865 10467  1009 11946\n",
      "   880  1058  1640   408 10863   258    18   584   134 10863 17899  2288\n",
      " 15681   865 10467     1    32 36026   381    20    46 17447  1403   426\n",
      "  9779   188  4220 10863     1   115   658   511    91     5 10863  1541\n",
      "   436  2257   131  2124  2367   626    22    67   112  4731  5337   300\n",
      "  1313 29470    18   626   547   878   655   685     4   444   190   537\n",
      "   131   677  3371  1224   779    54  1229   260     2    20     5 10863\n",
      "     4   570    73   468    30    20   239   695   151   269   108  7617\n",
      "   662  3514 10863     1 36027  1676     2   152   406  1512   287     4\n",
      "   946    20    48  1488  1215  2336    16   601     6    71   434   713\n",
      "  7167    16    46    20   194   435  3890  3466    46   105   263   487\n",
      "   246   282   118     4 19320 19321   355  1489]\n",
      "\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     9   272   212  3549  4509 10468   348     3   452\n",
      "   183    21   697  8411 10998  8646  1850  1081  4599   272   184   359\n",
      " 10468  2736   177     6   207     3   134  2507   105  1190   637   262\n",
      "  2544   187    84   208   669  1081  3317   232   746  3719  3878  4315\n",
      "   184   452   209   203   194    91     2 19917  2368  1337    79  6462\n",
      "   181    87   317   884     2   567 12471     5  2241    20    40   941\n",
      "  1337   431   697   359 10468   186  2779  1850  1081  4599   272   591\n",
      "   167   348    24   680  5218  1337 10999  5607]\n",
      "\n",
      "[ 6288  4709   308  9361  8885  3623  1933  7431  2248 11138  1524  2479\n",
      "  2087  2931  3016   949  1709  1242  1513 33436  1851  1594  2479  1395\n",
      "  9576 36028  4020  1686 50269   476   951 23473 62125 31270   180    63\n",
      "   237   321  5665 20502    70  1009  1253  1513 17036  6288  4709   463\n",
      "   412 16334   476     4   826     1 17900  9575  3791   791 14531  1661\n",
      "   480  9577  1450     9     2  1467  6243   658   464   520 17037  1021\n",
      "  5060  5630   279  2668 31270   896  1496   222   202     9    13  1525\n",
      "  1064  8047   639  1936    93   265     9  1253  1513  2177    24  1213\n",
      " 19918   226   377  2198 21883   285    70  1800 12472  5083  1530  1596\n",
      "    66   108 19919  1646   285 16676 12657   554  2685  9163 12855  3613\n",
      "  6966   564   388  2413    54   316   264   722  1994  1046  1275    58\n",
      "    33 31270 43715   888 14531   959   523  1166 31271  6498  2190   512\n",
      "  4021   223  8183  3842    17    52 16335  9900 36029  2025 16677  3994\n",
      "  4092  8113  6499  1217   353   983  1627   304   845   442   645  2651\n",
      "  2686   221    36   730   848    33  2651  2038    67   547   685   608\n",
      " 22662  3105  1170  2087   611    36   317  1847   351    50  9258  1828\n",
      " 18340 27876  4344 43716   589   763  1229  5262]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the reviews after padding has been completed. \n",
    "for i in range(3):\n",
    "    print(train_pad[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training and validation sets\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_pad, train.sentiment, test_size = 0.15, random_state = 2)\n",
    "x_test = test_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21250, 200)\n",
      "(3750, 200)\n",
      "(25000, 200)\n"
     ]
    }
   ],
   "source": [
    "# Inspect the shape of the data\n",
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size):\n",
    "    '''Create the batches for the training and validation data'''\n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_batches(x, batch_size):\n",
    "    '''Create the batches for the testing data'''\n",
    "    n_batches = len(x)//batch_size\n",
    "    x = x[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(n_words, embed_size, batch_size, lstm_size, num_layers, \n",
    "              dropout, learning_rate, multiple_fc, fc_units):\n",
    "    '''Build the Recurrent Neural Network'''\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "\n",
    "    with tf.name_scope('labels'):\n",
    "        labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    # Create the embeddings\n",
    "    with tf.name_scope(\"embeddings\"):\n",
    "        embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "        \n",
    "        #data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "        #data = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "\n",
    "    # Build the RNN layers\n",
    "    with tf.name_scope(\"RNN_layers\"):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    \n",
    "    # Set the initial state\n",
    "    with tf.name_scope(\"RNN_init_state\"):\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    # Run the data through the RNN layers\n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                                 initial_state=initial_state)    \n",
    "    \n",
    "    # Create the fully connected layers\n",
    "    with tf.name_scope(\"fully_connected\"):\n",
    "        \n",
    "        # Initialize the weights and biases\n",
    "        weights = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        \n",
    "        dense = tf.contrib.layers.fully_connected(outputs[:, -1],\n",
    "                                                  num_outputs = fc_units,\n",
    "                                                  activation_fn = tf.sigmoid,\n",
    "                                                  weights_initializer = weights,\n",
    "                                                  biases_initializer = biases)\n",
    "        dense = tf.contrib.layers.dropout(dense, keep_prob)\n",
    "        \n",
    "        # Depending on the iteration, use a second fully connected layer\n",
    "        if multiple_fc == True:\n",
    "            dense = tf.contrib.layers.fully_connected(dense,\n",
    "                                                      num_outputs = fc_units,\n",
    "                                                      activation_fn = tf.sigmoid,\n",
    "                                                      weights_initializer = weights,\n",
    "                                                      biases_initializer = biases)\n",
    "            dense = tf.contrib.layers.dropout(dense, keep_prob)\n",
    "    \n",
    "    # Make the predictions\n",
    "    with tf.name_scope('predictions'):\n",
    "        predictions = tf.contrib.layers.fully_connected(dense, \n",
    "                                                        num_outputs = 1, \n",
    "                                                        activation_fn=tf.sigmoid,\n",
    "                                                        weights_initializer = weights,\n",
    "                                                        biases_initializer = biases)\n",
    "        tf.summary.histogram('predictions', predictions)\n",
    "    \n",
    "    # Calculate the cost\n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.losses.mean_squared_error(labels, predictions)\n",
    "        tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    # Train the model\n",
    "    with tf.name_scope('train'):    \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # Determine the accuracy\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # Merge all of the summaries\n",
    "    merged = tf.summary.merge_all()    \n",
    "\n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs', 'labels', 'keep_prob', 'initial_state', 'final_state','accuracy',\n",
    "                    'predictions', 'cost', 'optimizer', 'merged']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, log_string):\n",
    "    '''Train the RNN'''\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Used to determine when to stop the training early\n",
    "        valid_loss_summary = []\n",
    "        \n",
    "        # Keep track of which batch iteration is being trained\n",
    "        iteration = 0\n",
    "\n",
    "        print()\n",
    "        print(\"Training Model: {}\".format(log_string))\n",
    "\n",
    "        train_writer = tf.summary.FileWriter('./logs/3/train/{}'.format(log_string), sess.graph)\n",
    "        valid_writer = tf.summary.FileWriter('./logs/3/valid/{}'.format(log_string))\n",
    "\n",
    "        for e in range(epochs):\n",
    "            state = sess.run(model.initial_state)\n",
    "            \n",
    "            # Record progress with each epoch\n",
    "            train_loss = []\n",
    "            train_acc = []\n",
    "            val_acc = []\n",
    "            val_loss = []\n",
    "\n",
    "            with tqdm(total=len(x_train)) as pbar:\n",
    "                for _, (x, y) in enumerate(get_batches(x_train, y_train, batch_size), 1):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.labels: y[:, None],\n",
    "                            model.keep_prob: dropout,\n",
    "                            model.initial_state: state}\n",
    "                    summary, loss, acc, state, _ = sess.run([model.merged, \n",
    "                                                             model.cost, \n",
    "                                                             model.accuracy, \n",
    "                                                             model.final_state, \n",
    "                                                             model.optimizer], \n",
    "                                                            feed_dict=feed)                \n",
    "                    \n",
    "                    # Record the loss and accuracy of each training batch\n",
    "                    train_loss.append(loss)\n",
    "                    train_acc.append(acc)\n",
    "                    \n",
    "                    # Record the progress of training\n",
    "                    train_writer.add_summary(summary, iteration)\n",
    "                    \n",
    "                    iteration += 1\n",
    "                    pbar.update(batch_size)\n",
    "            \n",
    "            # Average the training loss and accuracy of each epoch\n",
    "            avg_train_loss = np.mean(train_loss)\n",
    "            avg_train_acc = np.mean(train_acc) \n",
    "\n",
    "            val_state = sess.run(model.initial_state)\n",
    "            with tqdm(total=len(x_valid)) as pbar:\n",
    "                for x, y in get_batches(x_valid, y_valid, batch_size):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.labels: y[:, None],\n",
    "                            model.keep_prob: 1,\n",
    "                            model.initial_state: val_state}\n",
    "                    summary, batch_loss, batch_acc, val_state = sess.run([model.merged, \n",
    "                                                                          model.cost, \n",
    "                                                                          model.accuracy, \n",
    "                                                                          model.final_state], \n",
    "                                                                         feed_dict=feed)\n",
    "                    \n",
    "                    # Record the validation loss and accuracy of each epoch\n",
    "                    val_loss.append(batch_loss)\n",
    "                    val_acc.append(batch_acc)\n",
    "                    pbar.update(batch_size)\n",
    "            \n",
    "            # Average the validation loss and accuracy of each epoch\n",
    "            avg_valid_loss = np.mean(val_loss)    \n",
    "            avg_valid_acc = np.mean(val_acc)\n",
    "            valid_loss_summary.append(avg_valid_loss)\n",
    "            \n",
    "            # Record the validation data's progress\n",
    "            valid_writer.add_summary(summary, iteration)\n",
    "\n",
    "            # Print the progress of each epoch\n",
    "            print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                  \"Train Loss: {:.3f}\".format(avg_train_loss),\n",
    "                  \"Train Acc: {:.3f}\".format(avg_train_acc),\n",
    "                  \"Valid Loss: {:.3f}\".format(avg_valid_loss),\n",
    "                  \"Valid Acc: {:.3f}\".format(avg_valid_acc))\n",
    "\n",
    "            checkpoint = \"C:/Users/danie/Desktop/Checkpoints/sentiment_{}.ckpt\".format(log_string)\n",
    "            saver.save(sess, checkpoint)\n",
    "            '''# Stop training if the validation loss does not decrease after 3 epochs\n",
    "            if avg_valid_loss > min(valid_loss_summary):\n",
    "                print(\"No Improvement.\")\n",
    "                stop_early += 1\n",
    "                if stop_early == 3:\n",
    "                    break   \n",
    "            \n",
    "            # Reset stop_early if the validation loss finds a new low\n",
    "            # Save a checkpoint of the model\n",
    "            else:\n",
    "                print(\"New Record!\")\n",
    "                stop_early = 0\n",
    "                checkpoint = \"C:/Users/danie/Desktop/Checkpoints/sentiment_{}.ckpt\".format(log_string)\n",
    "                saver.save(sess, checkpoint)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default parameters of the model\n",
    "n_words = len(word_index)\n",
    "embed_size = 300\n",
    "batch_size = 250\n",
    "lstm_size = 128\n",
    "num_layers = 1\n",
    "dropout = 0.5\n",
    "learning_rate = 0.001\n",
    "epochs = 3\n",
    "multiple_fc = False\n",
    "fc_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model: ru=64,fcl=True,fcu=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [02:53<00:00, 124.83it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 366.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/3 Train Loss: 0.251 Train Acc: 0.542 Valid Loss: 0.176 Valid Acc: 0.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [02:53<00:00, 123.59it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 366.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3 Train Loss: 0.137 Train Acc: 0.820 Valid Loss: 0.125 Valid Acc: 0.837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [02:49<00:00, 123.28it/s]\n",
      "100%|██████████| 3750/3750 [00:10<00:00, 369.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/3 Train Loss: 0.084 Train Acc: 0.895 Valid Loss: 0.122 Valid Acc: 0.849\n",
      "\n",
      "Training Model: ru=64,fcl=True,fcu=256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21250/21250 [02:53<00:00, 138.65it/s]\n",
      "100%|██████████| 3750/3750 [00:09<00:00, 380.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/3 Train Loss: 0.249 Train Acc: 0.571 Valid Loss: 0.168 Valid Acc: 0.760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1250/21250 [00:09<02:34, 129.61it/s]"
     ]
    }
   ],
   "source": [
    "# Train the model with the desired tuning parameters\n",
    "for lstm_size in [64,128]:\n",
    "    for multiple_fc in [True, False]:\n",
    "        for fc_units in [128, 256]:\n",
    "            log_string = 'ru={},fcl={},fcu={}'.format(lstm_size,\n",
    "                                                      multiple_fc,\n",
    "                                                      fc_units)\n",
    "            model = build_rnn(n_words = n_words, \n",
    "                              embed_size = embed_size,\n",
    "                              batch_size = batch_size,\n",
    "                              lstm_size = lstm_size,\n",
    "                              num_layers = num_layers,\n",
    "                              dropout = dropout,\n",
    "                              learning_rate = learning_rate,\n",
    "                              multiple_fc = multiple_fc,\n",
    "                              fc_units = fc_units)            \n",
    "            train(model, epochs, log_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
